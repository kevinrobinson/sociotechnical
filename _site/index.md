# Fairness is a sociotechnical problem
Fairness in data science, machine learning, or any aspect of computer science is always challenging.  It involves balancing different stakeholders, translating between their vocabularies and perspectives, and finding ways to move forward that feels right to you and to everyone impacted by your work.

This makes the intake and discovery process in new projects particularly important.  It often involves coming into an unfamiliar domain and trying to understand it, while also building trust and confidence with your partners that your work can help them accomplish what they care about.


## I can haz PYthon cod enow!!
Most projects have limited time and budget.  But like distributed systems, cryptography algoriths, and authentication systems, fairness is often much more complicated than solving leetcode problems or running `pip install` and writing 3 lines of code :)

Each person and team needs to make their own tradeoffs.  But cutting corners on fairness can have the same kinds of severe unexpected consequences for a team as cutting corners on security.



## Try this right now
You can try each of these out in an hour and bring something to kick off a conversation with your team.

1. [Python functions for looking at the data](/todo)
2. [Talking about fairness](/todo)
3. [Noticing red flags](/todo)
4. [Introducing more sociotechnical context](/todo)
5. [Picking your first fairness metric](/todo)
6. [Pushing back against solutionism](/todo)
7. [Finding interdisciplinary feedback](/todo)
8. [Walking away professionally](/todo)

If you'd like more general guidelines and checklists for fairness in ML development:
- how to collaborate on ML framing and design ([People + AI Handbook](https://pair.withgoogle.com/guidebook/))
- how to co-design with people impacted by your work ([Design Justice Principles](https://designjustice.org/read-the-principles))
- how to audit datasets with datasheets ([research paper](https://arxiv.org/pdf/1803.09010.pdf))
- describing ML limitations with model cards ([example](https://modelcards.withgoogle.com/object-detection), [research paper](https://arxiv.org/pdf/1810.03993.pdf))
- fairness questions throughout the ML lifecycle ([see appendix of research paper](http://www.jennwv.com/papers/checklists.pdf))


## Domain-specific guides
1. [Facial recognition](/guides/facial_recognition)
2. [Criminal justice](/todo)
3. [Financial services](/todo)
4. [Education](/todo)
5. [Health care](/todo)
6. [HR](/todo)
7. [Media](/todo)


## Make a new friend!
These folks would love to take questions from data scientists trying to grow their work in fairness. They can help with things like:
- cutting through all the literature review
- recommending other collaborators in a specific field
- asking questions and spotting red flags

<div>
  <img style="width: 150px;" src="people/adrin.jpg" />
  <img style="width: 150px;" src="people/ken.jpg" />
  <img style="width: 150px;" src="people/jenn.png" />
  <img style="width: 150px;" src="people/vincent.jpg" />
  <img style="width: 150px;" src="people/hanna.jpg" />
  <img style="width: 150px;" src="people/roman.jpg" />
  <img style="width: 150px;" src="people/michael.jpg" />
  <img style="width: 150px;" src="people/miro.jpg" />
  <img style="width: 150px;" src="people/matthijs.jpg" />
</div>

DM on Twitter or come to Gitter!
