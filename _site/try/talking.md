# Talking about fairness

## Productive fairness conversations
Here's guidelines for productive team conversations about fairness:

1. **Deployment context**: Ground conversation in a real deployment context, not just a dataset.
2. **Real harms**: Focus on real harms to real people.  See [Blodget et al. (2020)](https://arxiv.org/abs/2005.14050)
3. **Sociotechnical**: Avoid abstraction traps.  See [Selbst et al. (2020)](https://andrewselbst.files.wordpress.com/2019/10/selbst-et-al-fairness-and-abstraction-in-sociotechnical-systems.pdf>).
4. **Substantiated**:  Discuss tradeoffs and compares alternatives.
5. **Translate to developers**: Speak the language of developers and data scientists.  Fits within the lifecycle of real practioner work.  See [Holstein et al (2019)](https://arxiv.org/pdf/1812.05239.pdf>), [Madaio et al. (2020)](http://www.jennwv.com/papers/checklists.pdf>).

For more detailed fairness checklists that cover the entire lifecycle of ML and data science work, see [Madaio et al. 2020](http://www.jennwv.com/papers/checklists.pdf) and [ABOUT ML](https://www.partnershiponai.org/about-ml/).


## When fairness is not a central concern
Figuring out how to talk about fairness, especially if it's not your core function or purpose on the team, can be tough.  Further, your own personal or ethical beliefs might not line up directly with the business needs of the client or team you're partnering with.  Here are some ways to frame the value for other stakeholders, adapted from *Challenges of incorporating algorithmic ‘fairness’ into practice* ([Cramer et al. 2019](https://algorithmicbiasinpractice.wordpress.com/slides/)):

1. Better product, serving wider audiences
2. Responsibility, social impact & PR
3. Legal & policy
4. Competitive, both proactive & reactive


## Visuals for talking about fairness
- max-min
- parity

