<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Picking your first fairness metric | sociotechnical</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Picking your first fairness metric" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/try/metrics.html" />
<meta property="og:url" content="http://localhost:4000/try/metrics.html" />
<meta property="og:site_name" content="sociotechnical" />
<script type="application/ld+json">
{"@type":"WebPage","url":"http://localhost:4000/try/metrics.html","headline":"Picking your first fairness metric","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <style>
    a[href$=todo]:hover, a[href$=todo]:focus  {
      /*opacity: 0.8;*/
      /*color:#069 !important;*/
    }
    /* https://github.com/pages-themes/minimal/blob/master/_sass/jekyll-theme-minimal.scss#L44 */
    a:hover, a:focus {
      color: orange !important;
      font-weight: normal !important; 
    }
  </style>

    <link rel="stylesheet" href="/assets/css/style.css?v=05463b97d9b5f8cf890127f67df1a6d14635694c">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <div style="margin-bottom: 40px; background: #ffa50026; border: 1px solid darkorange; padding: 20px;">
          <h1 style="margin-bottom: 10px;">
            <div><a href="http://localhost:4000/">sociotechnical</a></div>
          </h1>
          <div>Libraries, tools and guides for data scientists and software engineers that view ML fairness as a sociotechnical challenge</div>
        </div>

        
        

        <div>
          <div><a href="/index">I can haz PYthon cod enow!!</a></div>
          <div><a href="/notices">Fairness notices</a></div>
          <div><a href="/notebooks">Example notebooks</a></div>
          <div><a href="/scenarios">FAQ and scenarios</a></div>

          <div style="margin-top: 20px;">
            <div>Try this right now</div>

            <div style="margin-left: 10px;"><a href="/try/data">Python functions for reviewing data</a></div>
            <div style="margin-left: 10px;"><a href="/try/talking">Talking about fairness</a></div>
            <div style="margin-left: 10px;"><a href="/todo">Noticing red flags</a></div>
            <div style="margin-left: 10px;"><a href="/try/sociotechnical">Adding sociotechnical context</a></div>
            <div style="margin-left: 10px;"><a href="/try/metrics">Picking your first fairness metric</a></div>
            <div style="margin-left: 10px;"><a href="/todo">Finding interdisciplinary feedback</a></div>
            <div style="margin-left: 10px;"><a href="/try/pushing">Pushing back against tech solutionism</a></div>
            <div style="margin-left: 10px;"><a href="/todo">Walking away professionally</a></div>
          </div>

          <div style="margin-top: 20px;">
            <div>Domain-specific guides</div>
            <div style="margin-left: 10px;"><a href="/guides/facial_recognition">Facial recognition</a></div>
            <div style="margin-left: 10px;"><a href="/todo">Criminal justice</a></div>
            <div style="margin-left: 10px;"><a href="/todo">Financial services</a></div>
            <div style="margin-left: 10px;"><a href="/todo">Education</a></div>
            <div style="margin-left: 10px;"><a href="/guides/health">Health</a></div>
            <div style="margin-left: 10px;"><a href="/todo">HR</a></div>
            <div style="margin-left: 10px;"><a href="/todo">Media</a></div>
          </div>

          <div style="margin-top: 20px;">
            <div><a href="/todo">Make a new friend!</a></div>
          </div>

          <div>
            <p><small><a href="https://github.com/fairlearn/fairlearn">github repo</a></small></p>
          </div>
        </div>


      </header>
      <section>

      <h1 id="picking-your-first-fairness-metric">Picking your first fairness metric</h1>
<p>In some ways, choosing fairness metrics come with both all the existing challenges of choosing business or SLA metrics, but also with much harder sociotechnical challenges.</p>

<p>It can be helpful to think of these just like other software engineering tests - no unit test guarantees you are building great software, just like no type system guarntees you are shipping the right product.  Fairness metrics are only a small part of the work, but they can help teams in the same way that small tools like linters can.</p>

<h2 id="1-ensure-test-data-has-sufficient-representation">1. Ensure test data has sufficient representation</h2>

<h2 id="2-check-that-test-data-makes-sense-for-deployment-context">2. Check that test data makes sense for deployment context</h2>
<p>Before deploying, review whether the training data matches the deployment target as best you can.  If you are taking a dataset trained on Reddit posts and deploying it to analyze language for K12 student essays, you will probably run into problems.  But even subtle mismatches like differences between models trained on data from one time period, or on a sample of the deployment population can lead to issues.</p>

<blockquote>
  <p>(this visualization)</p>
</blockquote>

<h2 id="3-add-observability-and-monitoring-distribution-drift">3. Add observability and monitoring distribution drift</h2>
<p>Like with all production deployments, adding observability is critical for gaining visibility into how production services are performing.  For fairness questions, some ways to get started are:</p>
<ul>
  <li>monitor distribution of predictions over time</li>
  <li>check for out-of-distribution inputs</li>
</ul>

<blockquote>
  <p>“…we had a couple of deployments which regressed in serious ways which our error rate did not reflect…” ([Cramer et al. 2019])(https://algorithmicbiasinpractice.files.wordpress.com/2019/02/fat_2019tutorial_algorithmicbiasinpractice.pdf)</p>
</blockquote>

<h2 id="4-invite-input-from-diverse-stakeholders-on-what-matters-for-fairness">4. Invite input from diverse stakeholders on what matters for fairness</h2>
<p>This is often one of the most efficient and practical steps a team can take, but may involve bringing in external collaborators or partners.</p>

<p>Some starting points for questions are:</p>
<ul>
  <li>What happens if the system’s predictions are wrong?</li>
  <li>What subset of people would be harmed the most by wrong predictions?</li>
</ul>

<h2 id="5-choosing-specific-fairness-metrics">5. Choosing specific fairness metrics</h2>
<p>Two helpful metrics to get started with are false positive rate or false negative rate.  Beyond those, you can also look at them for particular subgroups (eg, across all elements of subgroups).</p>

<h2 id="6-group-fairness-metrics">6. Group fairness metrics</h2>
<p>After you’ve set up for first fairness metrics, it may also make sense to look at group fairness metrics.  The most common are “demographic parity” and “equal opportunity,” but tradeoffs in these choices often require more communication with stakeholders.  These are described in depth in <em>Equality of Opportunity in Supervised Learning</em> (<a href="https://arxiv.org/abs/1610.02413">Hardt et al. 2016</a>) and can be explored interactively in <em>Attacking discrimination with smarter machine learning</em> <a href="https://research.google.com/bigpicture/attacking-discrimination-in-ml/">Wattenberg et al. 2016</a>.</p>

<p>In the way way that other metrics like code coverage are only a small part of understanding the quality of a codebase, fairness metrics  are only a small component of the fairness work on a project.</p>


      


      </section>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>
