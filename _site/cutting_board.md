----
CUTTING BOARD below, ignore

## Understand specific hot-button topics
(If we get one paragraph, what are the top 3 *stories* everyone should know)
- ML designers can create systems that are racist or sexist without intending to or even knowing they have done this.  Watch this three minute video on a facial recognition (Gender Shades).
- There are many systems that are legal, but deeply unfair. (???)
- If the project involves labeling and classifying something, you are making choices about fairness. (???)



# Who to learn from
Practicing data scientists
-

Expanding your view of fairness
- Design Justice Principles

The important of sociotechnical context


History of fairness in computer science
- Trouble with Bias


# Where to learn
FAccT
Design Justice

ICML, AIES within AAAI, NeurIPS, ICLR, CHI


While human centered-AI primarily focuses on how humans will experience AI systems, and how their lives will be influenced by those AI systems, it also means considering




As with automated linting, testing, and security audits, teams choose the level of risk and rigor that makes sense for their project.  At this point, using HTTP web servers is almost always a bad idea!  But expecting every project in the world to have the same balance of unit tests and to pick the same linter rules and cycle for security audits wouldn't make sense.

   - With geospatial data, use `fl.discover_places` to discover potential problem areas where generalization issues can lead to fairness problems.
   - `fl.discover_slices` efficiently slices predictions by subgroups and shows slices that have have fairness issues.